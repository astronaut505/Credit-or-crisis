{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Cleaning and Preprocessing:**\n",
    "So many missing values, we will need to treat each column depending on the nature of the data in each column and the overall context of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ID','SSN']\n",
    "df = df.drop(columns=columns_to_drop) #<--- dropping columns that are not needed\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('O').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Customer_ID']             = df.Customer_ID.apply(lambda x: int(x[4:], 16))\n",
    "df['Month']                   = pd.to_datetime(df.Month, format='%B').dt.month\n",
    "df['Age']                     = df['Age'].astype(str).str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df['Annual_Income']           = df['Annual_Income'].str.replace(r'\\D', '', regex=True).astype(float)\n",
    "df['Num_of_Loan']             = df.Num_of_Loan.astype(str).str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df['Num_of_Delayed_Payment']  = df['Num_of_Delayed_Payment'].str.replace(r'\\D', '', regex=True).astype(float)\n",
    "df['Num_Credit_Inquiries']    = df['Num_Credit_Inquiries'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "df['Num_Credit_Inquiries']    = df['Num_Credit_Inquiries'].replace('', np.nan).astype(float)\n",
    "df['Changed_Credit_Limit']    = df['Changed_Credit_Limit'].str.replace(r'_', '0').astype(float)\n",
    "df['Outstanding_Debt']        = df['Outstanding_Debt'].str.replace(r'(\\d)_', r'\\1', regex=True).astype(float)\n",
    "df['Amount_invested_monthly'] = df['Amount_invested_monthly'].replace('__10000__', np.nan).astype(float)\n",
    "df['Monthly_Balance']         = df['Monthly_Balance'].replace('__-333333333333333333333333333__', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df.copy()\n",
    "df_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(data):\n",
    "    if data is np.NaN or not isinstance(data, str):\n",
    "        return data\n",
    "    else:\n",
    "        return str(data).strip('_ ,\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_check.applymap(text_cleaning).replace(['', 'nan', '!@9#%8', '#F%$D@*&8', 'NaN'], np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def FillMissingWithGroupMode(df, group_column, target_column):\n",
    "    # Function to convert None to NaN and fill NaN with the mode of the group\n",
    "    def fill_mode_per_group(data, group, column):\n",
    "        # Replace None with NaN\n",
    "        data[column] = data[column].replace([None], np.nan)\n",
    "        # Calculate and fill the mode for each group\n",
    "        filled_data = data.groupby(group)[column].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "        return filled_data\n",
    "\n",
    "    # Display before filling NaN\n",
    "    print(f'\\nBefore filling NaN in {target_column}:')\n",
    "    print(df[target_column].isna().sum(), \"missing values\")\n",
    "    print(df.groupby(group_column)[target_column].apply(list).head())\n",
    "\n",
    "    # Fill NaN values\n",
    "    df[target_column] = fill_mode_per_group(df, group_column, target_column)\n",
    "\n",
    "    # Display after filling NaN\n",
    "    print(f'\\nAfter filling NaN in {target_column}:')\n",
    "    print(df[target_column].isna().sum(), \"missing values\")\n",
    "    print(df.groupby(group_column)[target_column].apply(list).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Name\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Name\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Payment_Behaviour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Credit_Mix\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Credit_Mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Occupation\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Occupation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Type_of_Loan\n",
    "df['Type_of_Loan'] = df['Type_of_Loan'].apply(lambda x: x.lower().replace('and ', '').replace(', ', ',').strip() if pd.notna(x) else x)\n",
    "import re\n",
    "def get_Diff_Values_Colum(df_column, diff_value=[], sep=',', replace=''):\n",
    "    column = df_column.dropna()\n",
    "    for i in column:\n",
    "        if sep not in i and i not in diff_value:\n",
    "            diff_value.append(i)\n",
    "        else:\n",
    "            for data in map(lambda x:x.strip(), re.sub(replace, '', i).split(sep)):\n",
    "                if not data in diff_value:\n",
    "                    diff_value.append(data)\n",
    "    return dict(enumerate(sorted(diff_value)))\n",
    "df.groupby('Customer_ID')['Type_of_Loan'].value_counts(dropna=False)\n",
    "df['Type_of_Loan'].replace([np.NaN], 'No Data', inplace=True)\n",
    "get_Diff_Values_Colum(df['Type_of_Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Num_of_Delayed_Payment\n",
    "percentile_95 = df['Num_of_Delayed_Payment'].quantile(0.95)\n",
    "df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].apply(lambda x: percentile_95 if x > percentile_95 else x)\n",
    "\n",
    "df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median = df['Num_of_Delayed_Payment'].median()\n",
    "df['Num_of_Delayed_Payment'].fillna(overall_median, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Num_Credit_Inquiries\n",
    "percentile_95_inquiries = df['Num_Credit_Inquiries'].quantile(0.95)\n",
    "df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].apply(lambda x: percentile_95_inquiries if x > percentile_95_inquiries else x)\n",
    "\n",
    "df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_inquiries = df['Num_Credit_Inquiries'].median()\n",
    "df['Num_Credit_Inquiries'].fillna(overall_median_inquiries, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Credit_History_Age\n",
    "def convert_to_total_months(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "    parts = age_str.split(' ')\n",
    "    years = int(parts[0]) if parts[0].isdigit() else 0\n",
    "    months = int(parts[3]) if len(parts) > 3 and parts[3].isdigit() else 0\n",
    "    return years * 12 + months\n",
    "\n",
    "df['Credit_History_Age'] = df['Credit_History_Age'].apply(convert_to_total_months)\n",
    "\n",
    "df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_credit_history = df['Credit_History_Age'].median()\n",
    "df['Credit_History_Age'].fillna(overall_median_credit_history, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Customer_ID' and fill NaNs with the median per customer\n",
    "df['Amount_invested_monthly'] = df.groupby('Customer_ID')['Amount_invested_monthly'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# In case the entire 'Amount_invested_monthly' for a customer group is NaN, fill with overall median\n",
    "overall_median_investment = df['Amount_invested_monthly'].median()\n",
    "df['Amount_invested_monthly'].fillna(overall_median_investment, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Monthly_Balance\n",
    "df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_balance = df['Monthly_Balance'].median()\n",
    "df['Monthly_Balance'].fillna(overall_median_balance, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Monthly_Inhand_Salary (Each customer had stable income in dataset)\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Monthly_Inhand_Salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outlier_ages(group):\n",
    "    if len(group) > 1:\n",
    "        mode_age = group.mode()[0]\n",
    "        group = group.apply(lambda x: x if x == mode_age else np.nan)\n",
    "    return group\n",
    "\n",
    "df['Age'] = df.groupby('Customer_ID')['Age'].transform(replace_outlier_ages)\n",
    "\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outlier_loan(group):\n",
    "    if len(group) > 1:\n",
    "        mode_age = group.mode()[0]\n",
    "        group = group.apply(lambda x: x if x == mode_age else np.nan)\n",
    "    return group\n",
    "\n",
    "df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].transform(replace_outlier_loan)\n",
    "\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Num_of_Loan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0] #<--- missing values are from test dataset which we merged with train dataset\n",
    "# I realized that the Credit_Score column values from test set were NaN, so I will split the data into train and test sets based on the Credit_Score column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Exploratory Data Analysis (EDA) and Handling Extreme Outliers and error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "def detect_outliers(dataframe):\n",
    "    outlier_indices_dict = {}\n",
    "\n",
    "    # Loop over each column in the DataFrame\n",
    "    for column in dataframe.select_dtypes(include=[np.number]).columns:\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = dataframe[column].quantile(0.05)\n",
    "        Q3 = dataframe[column].quantile(0.95)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define bounds for outliers\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "        # Find outliers\n",
    "        outliers = dataframe[(dataframe[column] < lower_bound) | (dataframe[column] > upper_bound)]\n",
    "        outlier_indices_dict[column] = outliers.index.tolist()\n",
    "\n",
    "    return outlier_indices_dict\n",
    "\n",
    "# Run the function to detect outliers in all numerical columns\n",
    "outliers_dict = detect_outliers(df)\n",
    "\n",
    "# Example to print the outliers for a column\n",
    "for column, indices in outliers_dict.items():\n",
    "    print(f\"Outliers in column {column}: {len(indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calculate the typical income for each customer\n",
    "typical_incomes = df.groupby('Customer_ID')['Annual_Income'].median()\n",
    "\n",
    "# Merge the median income back to the original dataframe\n",
    "df = df.join(typical_incomes.rename('Median_Income'), on='Customer_ID')\n",
    "\n",
    "# Calculate the threshold for being considered an outlier\n",
    "threshold = 0.5  # You can adjust this threshold as needed\n",
    "df['Income_Lower_Threshold'] = df['Median_Income'] * (1 - threshold)\n",
    "df['Income_Upper_Threshold'] = df['Median_Income'] * (1 + threshold)\n",
    "\n",
    "# Replace outliers with the median income\n",
    "outlier_condition = (\n",
    "    (df['Annual_Income'] < df['Income_Lower_Threshold']) |\n",
    "    (df['Annual_Income'] > df['Income_Upper_Threshold'])\n",
    ")\n",
    "df.loc[outlier_condition, 'Annual_Income'] = df.loc[outlier_condition, 'Median_Income']\n",
    "\n",
    "# Drop the extra columns if you no longer need them\n",
    "df.drop(['Median_Income', 'Income_Lower_Threshold', 'Income_Upper_Threshold'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Annual_Income'].quantile(0.95)  # Extreme outliers\n",
    "\n",
    "df['Annual_Income'] = df['Annual_Income'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['Annual_Income'], bins=50, color='blue', edgecolor='black')  # Reduced number of bins for clarity\n",
    "plt.title('Capped Distribution of Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.hist(df['Age'], bins=60, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the upper and lower limits\n",
    "upper_limit = df['Num_Bank_Accounts'].quantile(0.95)  # For extreme outliers\n",
    "lower_limit = 0  # Minimum realistic value\n",
    "\n",
    "df['Num_Bank_Accounts'] = df['Num_Bank_Accounts'].apply(lambda x: min(max(x, lower_limit), upper_limit))\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.hist(df['Num_Bank_Accounts'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Num_Bank_Accounts')\n",
    "plt.xlabel('Num_Bank_Accounts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Outstanding_Debt')\n",
    "plt.show()\n",
    "#<--- more debt leads to lower credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Monthly_Balance')\n",
    "plt.show()\n",
    "\n",
    "#<--- more balance leads to higher credit score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (20, 12))\n",
    "sns.barplot(data = df, x = 'Occupation', y = 'Annual_Income')\n",
    "plt.show()\n",
    "\n",
    "#<--- this looks like not a realistic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the upper and lower limits\n",
    "upper_limit = df['Num_Credit_Card'].quantile(0.95)  # For extreme outliers\n",
    "lower_limit = 0  # Minimum realistic value\n",
    "\n",
    "df['Num_Credit_Card'] = df['Num_Credit_Card'].apply(lambda x: min(max(x, lower_limit), upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Num_Credit_Card'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Num_Credit_Card')\n",
    "plt.xlabel('Num_Credit_Card')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Num_Credit_Card')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Num_of_Loan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Interest_Rate'].quantile(0.95)  # Extreme outliers\n",
    "\n",
    "df['Interest_Rate'] = df['Interest_Rate'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Interest_Rate'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Interest_Rate')\n",
    "plt.xlabel('Interest_Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Total_EMI_per_month'].quantile(0.95)  # Extreme outliers\n",
    "\n",
    "df['Total_EMI_per_month'] = df['Total_EMI_per_month'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Total_EMI_per_month'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Total_EMI_per_month')\n",
    "plt.xlabel('Total_EMI_per_month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Amount_invested_monthly'].quantile(0.95)  # Extreme outliers\n",
    "\n",
    "df['Amount_invested_monthly'] = df['Amount_invested_monthly'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Amount_invested_monthly'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Amount_invested_monthly')\n",
    "plt.xlabel('Amount_invested_monthly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (20, 20))\n",
    "sns.countplot(data = df, x = 'Occupation', hue = 'Payment_of_Min_Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Feature Engineering:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns =df.select_dtypes('O').columns\n",
    "display(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_head = df.select_dtypes('O').head(5)\n",
    "categorical_columns_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Unique values count:\", df[col].nunique())\n",
    "    print(\"Value counts:\")\n",
    "    #print(df[col].value_counts(dropna=False).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Name', axis=1, inplace=True) #<--- dropping 'Name' column as it has too many unique values\n",
    "df = pd.get_dummies(df, columns=['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour'], drop_first=True)\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Type_of_Loan'].value_counts(dropna=False).head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "df['Type_of_Loan'] = df['Type_of_Loan'].str.split(',')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "type_of_loan_mlb = pd.DataFrame(mlb.fit_transform(df['Type_of_Loan']), columns=mlb.classes_, index=df.index)\n",
    "df = df.join(type_of_loan_mlb)\n",
    "\n",
    "df.drop('Type_of_Loan', axis=1, inplace=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes('number').columns\n",
    "display(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_head = df.select_dtypes('number').head(10)\n",
    "numerical_columns_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_functions = ['mean', 'median', 'max', 'min', 'std']\n",
    "aggregated_df = df.groupby('Customer_ID').agg({\n",
    "    'Annual_Income': agg_functions,\n",
    "    'Num_of_Loan': agg_functions,\n",
    "    'Outstanding_Debt': agg_functions,\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns except for 'Customer_ID'\n",
    "aggregated_df.columns = ['Customer_ID'] + ['{}_{}'.format(col[0], col[1]) for col in aggregated_df.columns[1:]]\n",
    "\n",
    "# Merge the DataFrames\n",
    "df = df.merge(aggregated_df, on='Customer_ID', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature as the ratio of Outstanding_Debt to Annual_Income\n",
    "df['Debt_Income_Ratio'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
    "\n",
    "# Calculate the ratio of Monthly_Balance to Monthly_Inhand_Salary\n",
    "df['Balance_Salary_Ratio'] = df['Monthly_Balance'] / df['Monthly_Inhand_Salary']\n",
    "\n",
    "# Calculate the ratio of Total_EMI_per_month to Monthly_Inhand_Salary\n",
    "df['EMI_Salary_Ratio'] = df['Total_EMI_per_month'] / df['Monthly_Inhand_Salary']\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_utilization_category(ratio):\n",
    "    if ratio < 30:\n",
    "        return 'Low'\n",
    "    elif ratio < 60:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Apply the function to create a new categorical feature\n",
    "df['Credit_Utilization_Category'] = df['Credit_Utilization_Ratio'].apply(credit_utilization_category)\n",
    "\n",
    "# One-hot encoding of the new categorical feature\n",
    "df = pd.get_dummies(df, columns=['Credit_Utilization_Category'], drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Splitting the data: reminder that original test.csv target variable was already dropped**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Dropping 'Customer_ID' and 'Month' from the dataframe\n",
    "df = df.drop(columns=['Customer_ID', 'Month', 'Annual_Income', 'Num_of_Loan', 'Outstanding_Debt'], errors='ignore')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = df.drop(columns=['Credit_Score'])\n",
    "y = df['Credit_Score']\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inf in X_train:\", X_train.isin([np.inf, -np.inf]).sum().sum())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Identify columns with infinite values\n",
    "inf_columns = X_train.columns[X_train.isin([np.inf, -np.inf]).any()].tolist()\n",
    "\n",
    "print(\"Columns with Infinite Values:\", inf_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Feature Selection: Correlation, RandomForest Importance, RFECV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "\n",
    "# 1. Correlation Analysis on Training Data\n",
    "corr_matrix = X_train.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title(\"Correlation Matrix for Training Data\")\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated variables (0.85 threshold)\n",
    "high_corr_var = [col for col in corr_matrix.columns if any(corr_matrix[col] > 0.85)]\n",
    "print(\"Highly correlated variables:\", high_corr_var)\n",
    "\n",
    "# 2. Feature Importance with Random Forest on Training Data\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=feature_importances['importance'], y=feature_importances.index)\n",
    "plt.title(\"Feature Importances in Training Data\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Recursive Feature Elimination with Cross-Validation (RFECV) on Training Data\n",
    "rfecv = RFECV(estimator=rf, step=5, cv=StratifiedKFold(5), scoring='accuracy', n_jobs=-1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], marker='o')\n",
    "plt.title(\"RFECV - Number of Features vs. CV Score\")\n",
    "plt.show()\n",
    "\n",
    "# Final selected features\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train is your DataFrame\n",
    "corr_matrix = X_train.corr()\n",
    "\n",
    "# Find pairs of highly correlated features\n",
    "high_corr_var = np.where(corr_matrix > 0.85)\n",
    "high_corr_pairs = [(corr_matrix.columns[x], corr_matrix.columns[y])\n",
    "                   for x, y in zip(*high_corr_var) if x != y and x < y]\n",
    "\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"High correlation between {pair[0]} and {pair[1]}: {corr_matrix.loc[pair[0], pair[1]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RandomForest model from RFECV (assuming rfecv.estimator_ is a RandomForest model)\n",
    "rf_model = rfecv.estimator_\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Map these importances to the corresponding features\n",
    "feature_importances = pd.Series(importances, index=X_train.columns[rfecv.support_]).sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 features\n",
    "feature_importances = feature_importances.head(20).index\n",
    "print(\"Top 10 features:\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### High training accuracy but low validation accuracy indicates overfitting\n",
    "### Let's try to reduce overfitting by tuning the hyperparameters of the Random Forest model\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#param_grid = {\n",
    "#    'n_estimators': [50, 100, 200],\n",
    "#    'max_depth': [None, 10, 20, 30],\n",
    "#    'min_samples_split': [2, 5, 10],\n",
    "#    'min_samples_leaf': [1, 2, 4],\n",
    "#    'max_features': ['sqrt']  ##<--- this is the default value. not sure\n",
    "#}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [4, 6, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train[selected_features], y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "#<--- print stuff\n",
    "y_train_pred = best_rf.predict(X_train[selected_features])\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Set Accuracy:\", train_accuracy)\n",
    "\n",
    "y_valid_pred = best_rf.predict(X_valid[selected_features])\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "print(\"Validation Set Accuracy:\", valid_accuracy)\n",
    "\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- cross validation scores\n",
    "cross_val_scores = cross_val_score(best_rf, X_train[selected_features], y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\", cross_val_scores)\n",
    "print(\"Mean cross-validation score:\", cross_val_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#<--- learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    estimator=rf,\n",
    "    X=X_train[selected_features],\n",
    "    y=y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.15)\n",
    "\n",
    "plt.plot(train_sizes, valid_mean, label='Cross-validation score', color='green', marker='o')\n",
    "plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, color='green', alpha=0.15)\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
