{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Cleaning and Preprocessing:**\n",
    "So many missing values, we will need to treat each column depending on the nature of the data in each column and the overall context of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ID','SSN']\n",
    "df = df.drop(columns=columns_to_drop) #<--- dropping columns that are not needed\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('O').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Customer_ID']             = df.Customer_ID.apply(lambda x: int(x[4:], 16))\n",
    "df['Month']                   = pd.to_datetime(df.Month, format='%B').dt.month\n",
    "df['Age']                     = df['Age'].astype(str).str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df['Annual_Income']           = df['Annual_Income'].str.replace(r'\\D', '', regex=True).astype(float)\n",
    "df['Num_of_Loan']             = df.Num_of_Loan.astype(str).str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df['Num_of_Delayed_Payment']  = df['Num_of_Delayed_Payment'].str.replace(r'\\D', '', regex=True).astype(float)\n",
    "df['Num_Credit_Inquiries']    = df['Num_Credit_Inquiries'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "df['Num_Credit_Inquiries']    = df['Num_Credit_Inquiries'].replace('', np.nan).astype(float)\n",
    "df['Changed_Credit_Limit']    = df['Changed_Credit_Limit'].str.replace(r'_', '0').astype(float)\n",
    "df['Outstanding_Debt']        = df['Outstanding_Debt'].str.replace(r'(\\d)_', r'\\1', regex=True).astype(float)\n",
    "df['Amount_invested_monthly'] = df['Amount_invested_monthly'].replace('__10000__', np.nan).astype(float)\n",
    "df['Monthly_Balance']         = df['Monthly_Balance'].replace('__-333333333333333333333333333__', np.nan).astype(float)\n",
    "\n",
    "#<--- there are some placeholders or typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df.copy()\n",
    "df_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(data):\n",
    "    if data is np.NaN or not isinstance(data, str):\n",
    "        return data\n",
    "    else:\n",
    "        return str(data).strip('_ ,\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_check.applymap(text_cleaning).replace(['', 'nan', '!@9#%8', '#F%$D@*&8', 'NaN'], np.NaN)\n",
    "\n",
    "#<--- cleaning the text for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def FillMissingWithGroupMode(df, group_column, target_column):\n",
    "    #<--- function to convert None to NaN and fill NaN with the mode of the group\n",
    "    def fill_mode_per_group(data, group, column):\n",
    "        data[column] = data[column].replace([None], np.nan)\n",
    "        filled_data = data.groupby(group)[column].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "        return filled_data\n",
    "\n",
    "    print(f'\\nBefore filling NaN in {target_column}:')\n",
    "    print(df[target_column].isna().sum(), \"missing values\")\n",
    "    print(df.groupby(group_column)[target_column].apply(list).head())\n",
    "\n",
    "    df[target_column] = fill_mode_per_group(df, group_column, target_column)\n",
    "\n",
    "    print(f'\\nAfter filling NaN in {target_column}:')\n",
    "    print(df[target_column].isna().sum(), \"missing values\")\n",
    "    print(df.groupby(group_column)[target_column].apply(list).head())\n",
    "\n",
    "#So idea is that, there are customers with multiple loan (or entries). same customers have missing values in some columns. which shouldn't change. Like Name, Age and etc.\n",
    "#We group them, scan, and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Name\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Payment_Behaviour\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Payment_Behaviour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Credit_Mix\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Credit_Mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Occupation\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Occupation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Type_of_Loan\n",
    "df['Type_of_Loan'] = df['Type_of_Loan'].apply(lambda x: x.lower().replace('and ', '').replace(', ', ',').strip() if pd.notna(x) else x)\n",
    "import re\n",
    "def get_Diff_Values_Colum(df_column, diff_value=[], sep=',', replace=''):\n",
    "    column = df_column.dropna()\n",
    "    for i in column:\n",
    "        if sep not in i and i not in diff_value:\n",
    "            diff_value.append(i)\n",
    "        else:\n",
    "            for data in map(lambda x:x.strip(), re.sub(replace, '', i).split(sep)):\n",
    "                if not data in diff_value:\n",
    "                    diff_value.append(data)\n",
    "    return dict(enumerate(sorted(diff_value)))\n",
    "df.groupby('Customer_ID')['Type_of_Loan'].value_counts(dropna=False)\n",
    "df['Type_of_Loan'].replace([np.NaN], 'No Data', inplace=True)\n",
    "get_Diff_Values_Colum(df['Type_of_Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Num_of_Delayed_Payment\n",
    "percentile_95 = df['Num_of_Delayed_Payment'].quantile(0.95)\n",
    "df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].apply(lambda x: percentile_95 if x > percentile_95 else x)\n",
    "\n",
    "df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median = df['Num_of_Delayed_Payment'].median()\n",
    "df['Num_of_Delayed_Payment'].fillna(overall_median, inplace=True)\n",
    "\n",
    "\n",
    "#<--- and for numerical, that might change in time, we fill them with median of the group (grouped by Customer ID, so same person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Num_Credit_Inquiries\n",
    "percentile_95_inquiries = df['Num_Credit_Inquiries'].quantile(0.95)\n",
    "df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].apply(lambda x: percentile_95_inquiries if x > percentile_95_inquiries else x)\n",
    "\n",
    "df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_inquiries = df['Num_Credit_Inquiries'].median()\n",
    "df['Num_Credit_Inquiries'].fillna(overall_median_inquiries, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Credit_History_Age\n",
    "def convert_to_total_months(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "    parts = age_str.split(' ')\n",
    "    years = int(parts[0]) if parts[0].isdigit() else 0\n",
    "    months = int(parts[3]) if len(parts) > 3 and parts[3].isdigit() else 0\n",
    "    return years * 12 + months\n",
    "\n",
    "df['Credit_History_Age'] = df['Credit_History_Age'].apply(convert_to_total_months)\n",
    "\n",
    "df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_credit_history = df['Credit_History_Age'].median()\n",
    "df['Credit_History_Age'].fillna(overall_median_credit_history, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Amount_invested_monthly\n",
    "df['Amount_invested_monthly'] = df.groupby('Customer_ID')['Amount_invested_monthly'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_investment = df['Amount_invested_monthly'].median()\n",
    "df['Amount_invested_monthly'].fillna(overall_median_investment, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<---Monthly_Balance\n",
    "df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "overall_median_balance = df['Monthly_Balance'].median()\n",
    "df['Monthly_Balance'].fillna(overall_median_balance, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Monthly_Inhand_Salary (Each customer had stable income in dataset)\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Monthly_Inhand_Salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- cleaning the Age column from unrealistic values, or different age values for same customer\n",
    "def replace_outlier_ages(group):\n",
    "    if len(group) > 1:\n",
    "        mode_age = group.mode()[0]\n",
    "        group = group.apply(lambda x: x if x == mode_age else np.nan)\n",
    "    return group\n",
    "\n",
    "df['Age'] = df.groupby('Customer_ID')['Age'].transform(replace_outlier_ages)\n",
    "\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- same for Num_of_Loan\n",
    "def replace_outlier_loan(group):\n",
    "    if len(group) > 1:\n",
    "        mode_age = group.mode()[0]\n",
    "        group = group.apply(lambda x: x if x == mode_age else np.nan)\n",
    "    return group\n",
    "\n",
    "df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].transform(replace_outlier_loan)\n",
    "\n",
    "FillMissingWithGroupMode(df, 'Customer_ID', 'Num_of_Loan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0] #<--- final check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Exploratory Data Analysis (EDA) and Handling Extreme Outliers and error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- Checking for extreme outliers. Since it is representing real world financial data, I assume that distribution would have long tail. So will use 0.05 and 0.95 quantiles to detect only extreme outliers.\n",
    "def detect_outliers(dataframe):\n",
    "    outlier_indices_dict = {}\n",
    "\n",
    "    for column in dataframe.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = dataframe[column].quantile(0.05) #<--- 0.05 and 0.95 quantiles\n",
    "        Q3 = dataframe[column].quantile(0.95)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "        outliers = dataframe[(dataframe[column] < lower_bound) | (dataframe[column] > upper_bound)]\n",
    "        outlier_indices_dict[column] = outliers.index.tolist()\n",
    "\n",
    "    return outlier_indices_dict\n",
    "\n",
    "outliers_dict = detect_outliers(df)\n",
    "\n",
    "for column, indices in outliers_dict.items():\n",
    "    print(f\"Outliers in column {column}: {len(indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea here is to detect errors or typos in dataset. For example, someone would have same Annual_Income for 5 entries, but one of them would be 100 times more than others. So we will replace them with median of the group (grouped by Customer_ID).\n",
    "\n",
    "\n",
    "#<--- Pre-calculate the typical income for each customer\n",
    "typical_incomes = df.groupby('Customer_ID')['Annual_Income'].median()\n",
    "\n",
    "#<--- merge the median income to original df\n",
    "df = df.join(typical_incomes.rename('Median_Income'), on='Customer_ID')\n",
    "\n",
    "#<--- calculate the threshold for being considered an outlier\n",
    "threshold = 0.5 #<--- 50% threshold.\n",
    "df['Income_Lower_Threshold'] = df['Median_Income'] * (1 - threshold)\n",
    "df['Income_Upper_Threshold'] = df['Median_Income'] * (1 + threshold)\n",
    "\n",
    "#<--- replace outliers with the median income. We use median because, person might have different income in time, but we assume that it is not changing drastically.\n",
    "outlier_condition = (\n",
    "    (df['Annual_Income'] < df['Income_Lower_Threshold']) |\n",
    "    (df['Annual_Income'] > df['Income_Upper_Threshold'])\n",
    ")\n",
    "df.loc[outlier_condition, 'Annual_Income'] = df.loc[outlier_condition, 'Median_Income']\n",
    "\n",
    "#<--- i really don't want to have too many columns. So if there are any unncessary columns, I will drop them. for futurre reference.\n",
    "df.drop(['Median_Income', 'Income_Lower_Threshold', 'Income_Upper_Threshold'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after cleaning errors, we deal with real outliers.\n",
    "\n",
    "upper_limit = df['Annual_Income'].quantile(0.95)  #<---  Extreme outliers\n",
    "\n",
    "df['Annual_Income'] = df['Annual_Income'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['Annual_Income'], bins=50, color='blue', edgecolor='black')\n",
    "plt.title('Capped Distribution of Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to do anything with Age column. it already looks good.\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.hist(df['Age'], bins=60, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing\n",
    "upper_limit = df['Num_Bank_Accounts'].quantile(0.95)\n",
    "lower_limit = 0\n",
    "\n",
    "df['Num_Bank_Accounts'] = df['Num_Bank_Accounts'].apply(lambda x: min(max(x, lower_limit), upper_limit))\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.hist(df['Num_Bank_Accounts'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Num_Bank_Accounts')\n",
    "plt.xlabel('Num_Bank_Accounts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Outstanding_Debt')\n",
    "plt.show()\n",
    "#<--- more debt leads to lower credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Monthly_Balance')\n",
    "plt.show()\n",
    "\n",
    "#<--- more balance leads to higher credit score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (20, 12))\n",
    "sns.barplot(data = df, x = 'Occupation', y = 'Annual_Income')\n",
    "plt.show()\n",
    "\n",
    "#<--- this looks like not a realistic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the upper and lower limits\n",
    "upper_limit = df['Num_Credit_Card'].quantile(0.95)  # For extreme outliers\n",
    "lower_limit = 0  # Minimum realistic value\n",
    "\n",
    "df['Num_Credit_Card'] = df['Num_Credit_Card'].apply(lambda x: min(max(x, lower_limit), upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Num_Credit_Card'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Num_Credit_Card')\n",
    "plt.xlabel('Num_Credit_Card')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Num_Credit_Card')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "sns.boxplot(data = df,  x = 'Credit_Score', y = 'Num_of_Loan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Interest_Rate'].quantile(0.95)  #<--- Extreme outliers\n",
    "\n",
    "df['Interest_Rate'] = df['Interest_Rate'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Interest_Rate'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Interest_Rate')\n",
    "plt.xlabel('Interest_Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Total_EMI_per_month'].quantile(0.95)  #<--- Extreme outliers\n",
    "\n",
    "df['Total_EMI_per_month'] = df['Total_EMI_per_month'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Total_EMI_per_month'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Total_EMI_per_month')\n",
    "plt.xlabel('Total_EMI_per_month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df['Amount_invested_monthly'].quantile(0.95)  #<--- Extreme outliers\n",
    "\n",
    "df['Amount_invested_monthly'] = df['Amount_invested_monthly'].apply(lambda x: min(x, upper_limit))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df['Amount_invested_monthly'], bins=11, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Amount_invested_monthly')\n",
    "plt.xlabel('Amount_invested_monthly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "sns.countplot(data = df, x = 'Occupation', hue = 'Payment_of_Min_Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Feature Engineering:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns =df.select_dtypes('O').columns\n",
    "display(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_head = df.select_dtypes('O').head(5)\n",
    "categorical_columns_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Unique values count:\", df[col].nunique())\n",
    "    print(\"Value counts:\")\n",
    "    #print(df[col].value_counts(dropna=False).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Name', axis=1, inplace=True) #<--- dropping 'Name' column as it has too many unique values and it is not needed for our model.\n",
    "df = pd.get_dummies(df, columns=['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour'], drop_first=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Type_of_Loan'].value_counts(dropna=False).head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "df['Type_of_Loan'] = df['Type_of_Loan'].str.split(',')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "type_of_loan_mlb = pd.DataFrame(mlb.fit_transform(df['Type_of_Loan']), columns=mlb.classes_, index=df.index)\n",
    "df = df.join(type_of_loan_mlb)\n",
    "\n",
    "df.drop('Type_of_Loan', axis=1, inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "#<--- i am 100% sure that there is smarter way to do this. But i just decided not to worry about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes('number').columns\n",
    "display(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_head = df.select_dtypes('number').head(10)\n",
    "numerical_columns_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg_functions = ['mean', 'median', 'max', 'min', 'std']\n",
    "#aggregated_df = df.groupby('Customer_ID').agg({\n",
    "#    'Annual_Income': agg_functions,\n",
    "#    'Num_of_Loan': agg_functions,\n",
    "#    'Outstanding_Debt': agg_functions,\n",
    "#}).reset_index()\n",
    "#\n",
    "## Rename columns except for 'Customer_ID'\n",
    "#aggregated_df.columns = ['Customer_ID'] + ['{}_{}'.format(col[0], col[1]) for col in aggregated_df.columns[1:]]\n",
    "#\n",
    "## Merge the DataFrames\n",
    "#df = df.merge(aggregated_df, on='Customer_ID', how='left')\n",
    "#df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#<--- this I am sure is very important. but these features were correlated with each other. It would take too much time to find out which one is more important. So I decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--- I already have way too many features. So, I will not add even more features. But I will create some ratios, which might be useful.\n",
    "\n",
    "df['Debt_Income_Ratio'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
    "df['Balance_Salary_Ratio'] = df['Monthly_Balance'] / df['Monthly_Inhand_Salary']\n",
    "df['EMI_Salary_Ratio'] = df['Total_EMI_per_month'] / df['Monthly_Inhand_Salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_utilization_category(ratio):\n",
    "    if ratio < 30:\n",
    "        return 'Low'\n",
    "    elif ratio < 60:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['Credit_Utilization_Category'] = df['Credit_Utilization_Ratio'].apply(credit_utilization_category) #<--- applying\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Credit_Utilization_Category'], drop_first=True) #<--- encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Splitting the data: reminder that original test.csv target variable was already dropped**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#<--- unnecessary columns\n",
    "df = df.drop(columns=['Customer_ID', 'Month'], errors='ignore')\n",
    "#df = df.drop(columns=['Customer_ID', 'Month', 'Annual_Income', 'Num_of_Loan', 'Outstanding_Debt'], errors='ignore')   <---- drop these when using agg. original features are correlated with agg statistics.\n",
    "\n",
    "X = df.drop(columns=['Credit_Score'])\n",
    "y = df['Credit_Score']\n",
    "\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "#<--- splliting again after SMOTE\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inf in X_train:\", X_train.isin([np.inf, -np.inf]).sum().sum())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#<--- Identify columns with infinite values\n",
    "inf_columns = X_train.columns[X_train.isin([np.inf, -np.inf]).any()].tolist()\n",
    "\n",
    "print(\"Columns with Infinite Values:\", inf_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Feature Selection: Correlation, RandomForest Importance, RFECV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#<--- 1. correlation analysis on training\n",
    "corr_matrix = X_train.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title(\"correlation matrix for Ttraining\")\n",
    "plt.show()\n",
    "\n",
    "#<--- highly correlated variables (0.85 threshold)\n",
    "high_corr_var = [col for col in corr_matrix.columns if any(corr_matrix[col] > 0.85)]\n",
    "print(\"highly correlated variables:\", high_corr_var)\n",
    "\n",
    "#<--- 2. Feature Importance with Random Forest on training\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=777)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=feature_importances['importance'], y=feature_importances.index)\n",
    "plt.title(\"feature importances in training\")\n",
    "plt.show()\n",
    "\n",
    "#<--- 3. RFECV on training\n",
    "rfecv = RFECV(estimator=rf, step=5, cv=StratifiedKFold(5), scoring='accuracy', n_jobs=-1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], marker='o')\n",
    "plt.title(\"RFECV - n of features vs. CV score\")\n",
    "plt.show()\n",
    "\n",
    "#<--- final selected features\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "print(\"selected features:\", selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr()\n",
    "\n",
    "#<--- find pairs of highly correlated features (to check if engineered features are correlated with original features)\n",
    "high_corr_var = np.where(corr_matrix > 0.85)\n",
    "high_corr_pairs = [(corr_matrix.columns[x], corr_matrix.columns[y])\n",
    "                   for x, y in zip(*high_corr_var) if x != y and x < y]\n",
    "\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"High correlation between {pair[0]} and {pair[1]}: {corr_matrix.loc[pair[0], pair[1]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Model Building and Tunning: Logistic Regression (baseline model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#<--- Scaling the features (only selected features, only for this model)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_valid_scaled = scaler.transform(X_valid[selected_features])\n",
    "\n",
    "#<--- training the Logistic Regression model\n",
    "model = LogisticRegression(random_state=777)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#<--- predictions\n",
    "y_pred_train_lr = model.predict(X_train_scaled)  # Use scaled data\n",
    "y_pred_lr = model.predict(X_valid_scaled)        # Use scaled data\n",
    "\n",
    "#<--- accuracy scores\n",
    "train_accuracy_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "valid_accuracy_lr = accuracy_score(y_valid, y_pred_lr)\n",
    "class_names = y.unique()\n",
    "#<--- classification report\n",
    "print(f\"Logistic Regression - Training Set Accuracy: {train_accuracy_lr:.4f}\")\n",
    "print(f\"Logistic Regression - Validation Set Accuracy: {valid_accuracy_lr:.4f}\")\n",
    "print(classification_report(y_valid, y_pred_lr, target_names=class_names))\n",
    "\n",
    "#<--- confusion matrix\n",
    "conf_matrix_lr = confusion_matrix(y_valid, y_pred_lr, labels=class_names)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix_lr, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Logistic Regression - Confusion Matrix with Class Names')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Model Building and Tunning: Decision Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#<--- initial parameters\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 15, 20],  #<--- depth reduced to avoid overfitting (see previous commit for metrics)\n",
    "    'min_samples_split': [4, 10, 20],  #<--- same\n",
    "    'min_samples_leaf': [4, 6, 8]  #<--- same\n",
    "}\n",
    "\n",
    "#<--- model\n",
    "decision_tree = DecisionTreeClassifier(random_state=777)\n",
    "\n",
    "#<--- grid\n",
    "grid_search_dt = GridSearchCV(decision_tree, param_grid_dt, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_dt.fit(X_train[selected_features], y_train)\n",
    "\n",
    "#<--- get and store the best parameters\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "print(\"Best parameters found by grid search for Decision Tree:\", best_params_dt)\n",
    "best_model_dt = grid_search_dt.best_estimator_\n",
    "\n",
    "#<--- Predictions and Evaluation\n",
    "y_pred_train_dt = best_model_dt.predict(X_train[selected_features])\n",
    "y_pred_dt = best_model_dt.predict(X_valid[selected_features])\n",
    "class_names = y.unique()\n",
    "\n",
    "#<--- training set accuracy\n",
    "train_accuracy_dt = accuracy_score(y_train, y_pred_train_dt)\n",
    "print(f\"Decision Tree - Training Set Accuracy: {train_accuracy_dt:.4f}\")\n",
    "\n",
    "#<--- vlaidation set accuracy\n",
    "valid_accuracy_dt = accuracy_score(y_valid, y_pred_dt)\n",
    "print(f\"Decision Tree - Validation Set Accuracy: {valid_accuracy_dt:.4f}\")\n",
    "\n",
    "#<--- classification results\n",
    "print(classification_report(y_valid, y_pred_dt, target_names=class_names))\n",
    "\n",
    "#<--- matrix\n",
    "conf_matrix_dt = confusion_matrix(y_valid, y_pred_dt, labels=class_names)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix_dt, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Decision Tree - Confusion Matrix with Class Names')\n",
    "plt.show()\n",
    "\n",
    "# we can see that model is slightly overfitting. I will not try to fix it, because it is not that bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Model Building and Tunning: Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#param_grid = {\n",
    "#    'n_estimators': [50, 100, 200],\n",
    "#    'max_depth': [None, 10, 20, 30],\n",
    "#    'min_samples_split': [2, 5, 10],\n",
    "#    'min_samples_leaf': [1, 2, 4],\n",
    "#    'max_features': ['sqrt']  ##<--- this was causing overfitting. So I decided tune it.\n",
    "#}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [4, 6, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=777)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train[selected_features], y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "#<--- print stuff\n",
    "#<--- Predictions and Evaluation\n",
    "y_train_pred_rf = best_rf.predict(X_train[selected_features])\n",
    "\n",
    "#<--- on the validation set\n",
    "y_valid_pred_rf = best_rf.predict(X_valid[selected_features])\n",
    "class_names = y.unique()\n",
    "\n",
    "#<--- training set acc\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"Random Forest - Training Set Accuracy: {train_accuracy_rf:.4f}\")\n",
    "\n",
    "#<--- validation set accuracy\n",
    "valid_accuracy_rf = accuracy_score(y_valid, y_valid_pred_rf)\n",
    "print(f\"Random Forest - Validation Set Accuracy: {valid_accuracy_rf:.4f}\")\n",
    "\n",
    "print(classification_report(y_valid, y_valid_pred_rf, target_names=class_names))\n",
    "\n",
    "#<--- matrix\n",
    "conf_matrix_rf = confusion_matrix(y_valid, y_valid_pred_rf, labels=class_names)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Random Forest - Confusion Matrix with Class Names')\n",
    "plt.show()\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "\n",
    "#<--- took almost 1 hour to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#<--- just checking the learning curve to see if there is overfitting or underfitting. I think it is fine.\n",
    "rf = RandomForestClassifier(max_depth=20, max_features='sqrt', min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1)\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    estimator=rf,\n",
    "    X=X_train[selected_features],\n",
    "    y=y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.15)\n",
    "plt.plot(train_sizes, valid_mean, label='Cross-validation score', color='green', marker='o')\n",
    "plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, color='green', alpha=0.15)\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Model Building and Tunning: LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'n_estimators': [50, 150, 200],\n",
    "    'num_leaves': [6, 16, 24, 31],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['multiclass'],\n",
    "    'colsample_bytree': [0.65, 0.75],\n",
    "    'subsample': [0.7, 0.75],\n",
    "    'reg_alpha': [1, 1.2],\n",
    "    'reg_lambda': [1, 1.2, 1.4],\n",
    "}\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(objective='multiclass', num_class=len(y.unique()), verbosity=-1, random_state=777) #<--- multiclass because we have 3 classes\n",
    "\n",
    "#<--- grid search\n",
    "grid_search = GridSearchCV(lgb_clf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train[selected_features], y_train)\n",
    "\n",
    "#<--- saving the best parameters so I don't need to run grid search every time\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by grid search:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_valid[selected_features])\n",
    "\n",
    "#<--- decided to make sepeare function for confusion matrix and performance metrics\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_valid, y_pred, average='macro')\n",
    "\n",
    "\n",
    "class_names = y.unique()\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, best_model.predict(X_train[selected_features]))\n",
    "print(f\"Training Set Accuracy: {train_accuracy:.4f}\")\n",
    "valid_accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Validation Set Accuracy: {valid_accuracy:.4f}\")\n",
    "print(classification_report(y_valid, y_pred, target_names=class_names))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred, labels=class_names)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix with Class Names')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, this project looks finished. \n",
    "\n",
    "For **Decision Trees I have got 0.80 average accuracy**.  Slight overfitting, giving the computational cost increased a lot, I will not investigate further. \n",
    "\n",
    "\n",
    "I've got average **0.83 accuracy on RFClassifier**, slightly lower than training set accuracy, so little bit overfitting. \n",
    "\n",
    "**For LightGBM, I've got 0.75 prediction accuracy**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
